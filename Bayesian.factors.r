cat("
üß† 1Ô∏è‚É£ What is a Bayes Factor?

A Bayes factor (BF) quantifies how much more likely the data are under one hypothesis than another.

BF‚ÇÅ‚ÇÇ = P(Data | H‚ÇÅ) / P(Data | H‚ÇÇ)

Where:

BF‚ÇÅ‚ÇÇ = Bayes factor favoring H‚ÇÅ over H‚ÇÇ
P(Data | H‚ÇÅ) = Probability of observing the data if H‚ÇÅ is true
P(Data | H‚ÇÇ) = Probability of observing the data if H‚ÇÇ is true

Simple Example
Suppose you flip a coin 10 times and get 8 heads.

H‚ÇÅ: The coin is fair (p = 0.5)
H‚ÇÇ: The coin is biased (p = 0.7)

Calculate how likely '8 heads out of 10' is under each hypothesis:

")

# Probability under H1 (fair coin)
p_data_h1 <- dbinom(8, size = 10, prob = 0.5)  # = 0.044
p_data_h1

# Probability under H2 (biased coin)
p_data_h2 <- dbinom(8, size = 10, prob = 0.7)  # = 0.233
p_data_h2

# Bayes factor
BF <- p_data_h2 / p_data_h1  # = 5.3
print(BF)

cat("

The Binomial Distribution

Density, distribution function, quantile function and random generation 
for the binomial distribution with parameters size and prob. 

üëâ A Bayes Factor of 10 means:

The data are 10√ó more likely under the alternative than under the null hypothesis.

A Bayes Factor of 0.1 means:

The data are 10√ó more likely under the null than under the alternative

üß© 3Ô∏è‚É£ Relationship to p-values

| Concept          | What it measures                                                     | Interpretation        |        |                                                 |
| ---------------- | -------------------------------------------------------------------- | --------------------- | ------ | ----------------------------------------------- |
| **p-value**      | Probability of seeing data *as extreme* as observed if (H_0) is true | Reject (H_0) if small |        |                                                 |
| **Bayes Factor** | Ratio of likelihoods ( P(D                                           | H_1)/P(D              | H_0) ) | How much more the data support (H_1) over (H_0) |


")

# ?dbinom

cat("

| Bayes Factor ( BF_{10} ) | Evidence for ( H_1 ) (Alternative)        |
| ------------------------ | ----------------------------------------- |
| **> 100**                | Decisive evidence                         |
| **30 ‚Äì 100**             | Very strong evidence                      |
| **10 ‚Äì 30**              | Strong evidence                           |
| **3 ‚Äì 10**               | Moderate evidence                         |
| **1 ‚Äì 3**                | Weak evidence                             |
| **= 1**                  | No preference (data equally support both) |
| **< 1/3**                | Moderate evidence for ( H_0 )             |
| **< 1/10**               | Strong evidence for ( H_0 )               |
| **< 1/30**               | Very strong evidence for ( H_0 )          |

üëâ A Bayes Factor of 10 means:

The data are 10√ó more likely under the alternative than under the null hypothesis.

A Bayes Factor of 0.1 means:

The data are 10√ó more likely under the null than under the alternative

")




library(BayesFactor)

# Simulate data: test scores in control vs treatment groups
set.seed(123)
control <- rnorm(30, mean = 75, sd = 10)
treat   <- rnorm(30, mean = 80, sd = 10)   # treatment improves by +5 points

# === Classical t-test ===
t_test_result <- t.test(treat, control)
cat("\nt test result:\n")
t_test_result

bf_result <- ttestBF(x = treat, y = control)
cat("\nbayesian factors:\n")
bf_result


# ?ttestBF

boxplot(control, treat,
        names = c("Control", "Treatment"),
        col = c("lightblue", "lightgreen"),
        main = "Test Scores by Group")

cat("üß™ 1Ô∏è‚É£ Classical t-test 

‚úÖ Interpretation:

The t-statistic (3.08) shows a strong difference between treatment and control.

p-value = 0.003 ‚Üí much smaller than 0.05 ‚Üí statistically significant.
‚Üí We reject the null hypothesis (no difference in means).

95% Confidence Interval = [2.54, 11.97] means the true difference in means is very likely between +2.5 and +12 points.

Means:

Treatment group mean = 81.78

Control group mean = 74.53
‚Üí On average, the treatment group scored ‚âà7.25 points higher.

")

cat(" üßÆ 2Ô∏è‚É£ Bayesian Analysis

‚úÖ Interpretation:

The Bayes Factor (BF‚ÇÅ‚ÇÄ = 12.03) means:

The observed data are 12 times more likely under the alternative hypothesis 
(that there is a true difference in means) than under the null hypothesis (no difference).

This corresponds to ‚Äústrong evidence‚Äù for the alternative hypothesis (H‚ÇÅ).
")

cat("

| Bayes Factor (BF‚ÇÅ‚ÇÄ) | Interpretation            |
| ------------------- | ------------------------- |
| 1‚Äì3                 | Anecdotal evidence for H‚ÇÅ |
| 3‚Äì10                | Moderate evidence         |
| **10‚Äì30**           | **Strong evidence**       |
| 30‚Äì100              | Very strong evidence      |
| >100                | Decisive evidence         |

| Test                           | Result                   | Evidence Strength    | Conclusion                            |
| ------------------------------ | ------------------------ | -------------------- | ------------------------------------- |
| **t-test (p = 0.003)**         | Significant difference   | Strong (frequentist) | Treatment effect likely real          |
| **Bayes Factor (BF‚ÇÅ‚ÇÄ = 12.0)** | 12√ó more likely under H‚ÇÅ | Strong (Bayesian)    | Data strongly support real difference |
| **Mean difference = +7.25**    | CI [2.54, 11.97]         | ‚Äî                    | Treatment improves mean outcome       |

")

cat("

‚úÖ 4Ô∏è‚É£ In plain English

Both the frequentist and Bayesian analyses agree:
The treatment group performed significantly better than the control group.

Classical test: p = 0.003 ‚Üí statistically significant.

Bayesian test: BF = 12 ‚Üí data are 12√ó more likely if the treatment really has an effect.

Therefore, there is strong, converging evidence that the treatment improves outcomes by roughly 7 points on average.

")



cat("LIKELIHOOD")

cat("

üß† 1Ô∏è‚É£ What is ‚ÄúLikelihood‚Äù?

Likelihood tells us how likely the observed data are, given a specific model or parameter value.

Formally:

L(Œ∏‚à£data)=P(data‚à£Œ∏)

Œ∏ = model parameter(s) (e.g., mean, variance, regression coefficients)

data = what we actually observed

‚öñÔ∏è In words:

THE LIKELIHOOD IS NOT THE PROBABILITY OF THE PARAMETERS ‚Äî IT‚ÄôS THE PROBABILITY OF THE DATA, GIVEN THE PARAMETERS. ‚úÖ

üéØ 2Ô∏è‚É£ Example: Tossing a coin

Suppose you toss a coin 10 times and get 7 heads.

You want to know how likely that is for different values of 

Œ∏ (the probability of heads).

The likelihood for a binomial model is [ FORMULA] : 

| Œ∏ (probability of heads) | Likelihood ( L(\theta) ) |
| ------------------------ | ------------------------ |
| 0.2                      | 0.00079                  |
| 0.5                      | 0.117                    |
| **0.7**                  | **0.267**                |
| 0.9                      | 0.012                    |

‚úÖ The highest likelihood is at Œ∏ = 0.7
‚Üí So the data are most consistent with a coin that lands heads 70% of the time.

üßÆ 3Ô∏è‚É£ Likelihood vs Probability

| Concept         | What varies                 | Meaning                                             |
| --------------- | --------------------------- | --------------------------------------------------- |
| **Probability** | Data vary, parameters fixed | ‚ÄúGiven Œ∏, how likely are these data?‚Äù               |
| **Likelihood**  | Parameters vary, data fixed | ‚ÄúGiven these data, which Œ∏ makes them most likely?‚Äù |

So in inference, we treat the data as fixed and ask:

WHICH PARAMETER VALUE MAKES THE DATA MOST PLAUSIBLE? ‚úÖ

üìä 4Ô∏è‚É£ Maximum Likelihood Estimation (MLE)

The MLE is the parameter value that maximizes the likelihood function. 

It is the cornerstone of frequentist estimation and the starting point for most machine learning models 
(logistic regression, neural nets, etc.).

So the likelihood is the bridge connecting data to posterior inference in Bayesian statistics.

")

cat("

üß™ Example: Coin toss likelihood

We toss a coin 10 times and get 7 heads.

We want to see for which value of 

Œ∏ (probability of heads) the data are most likely ")

# 1Ô∏è‚É£ Simulated experiment
n <- 10          # number of tosses
k <- 7           # number of heads observed

# 2Ô∏è‚É£ Likelihood function for Binomial model
theta <- seq(0, 1, length.out = 200)
likelihood <- dbinom(k, size = n, prob = theta)

head(theta)
head(likelihood)


# 3Ô∏è‚É£ Normalize (optional, for nicer plotting)
likelihood <- likelihood / max(likelihood)

# 4Ô∏è‚É£ Plot likelihood curve
plot(theta, likelihood, type = "l", lwd = 3, col = "blue",
     main = "Likelihood Function for 7 Heads in 10 Tosses",
     xlab = expression(theta),
     ylab = "Relative Likelihood")

# 5Ô∏è‚É£ Mark the Maximum Likelihood Estimate (MLE)
theta_hat <- theta[which.max(likelihood)]
abline(v = theta_hat, col = "red", lwd = 2, lty = 2)
text(theta_hat, 0.9, labels = paste("MLE =", round(theta_hat, 2)),
     pos = 4, col = "red")

cat("

| Concept                      | Meaning                                                                   |
| ---------------------------- | ------------------------------------------------------------------------- |
| **Œ∏ (probability of heads)** | Parameter we‚Äôre estimating                                                |
| **Likelihood curve**         | How plausible each Œ∏ value is given our data (7 heads / 10 tosses)        |
| **Peak (Œ∏ÃÇ = 0.7)**          | The data are most consistent with a coin that lands heads 70% of the time |
| **Likelihood width**         | Reflects uncertainty ‚Äî flatter curve = more uncertainty                   |

Interpret :

The MLE = observed success rate (0.7)

")

cat("

| Concept                 | Meaning                                                            |
| ----------------------- | ------------------------------------------------------------------ |
| **Likelihood function** | Probability of data given parameters                               |
| **MLE**                 | Parameter values that maximize likelihood                          |
| **Each distribution**   | Has its own formula for the likelihood and its own closed-form MLE |
| **In ML**               | Training = maximizing (log) likelihood = minimizing loss           |

")

cat("

| Model               | Parameter(s)               | MLE finds‚Ä¶                                                               |
| ------------------- | -------------------------- | ------------------------------------------------------------------------ |
| Binomial            | Œ∏ = probability of success | Œ∏ that makes observed successes most likely                              |
| Normal              | Œº, œÉ¬≤                      | Mean and variance that best fit data                                     |
| Linear regression   | Œ≤ coefficients             | Values that maximize likelihood (equivalent to minimizing squared error) |
| Logistic regression | Œ≤ coefficients             | Coefficients that maximize likelihood of observed labels                 |

")








